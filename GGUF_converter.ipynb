{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3dc19d5-53cf-466b-ad66-6de8d80ab7e1",
   "metadata": {},
   "source": [
    "# GGUF Converter Notebook\n",
    "\n",
    "Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside however is that you need to convert models to a format that's supported by Llama.cpp, which is now the GGUF file format.\n",
    "\n",
    "This notebooks aims to be a guide to convert your model weigth/binaries into a GGUF file format compatible with llama.cpp\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- https://www.substratus.ai/blog/converting-hf-model-gguf-model/\n",
    "\n",
    "- https://github.com/ggerganov/llama.cpp/discussions/2948\n",
    "\n",
    "- https://hackernoon.com/the-cheapskates-guide-to-fine-tuning-llama-2-and-running-it-on-your-laptop\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- https://stackoverflow.com/questions/37890898/how-to-set-env-variable-in-jupyter-notebook\n",
    "\n",
    "- https://unix.stackexchange.com/questions/558350/is-there-in-bash-a-builtin-command-to-get-the-absolute-path-of-a-relative-file\n",
    "\n",
    "- https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html\n",
    "\n",
    "- https://cgold.readthedocs.io/en/latest/first-step/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e8dc4-2e70-4748-b010-166a64af8e7c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a771eeb8-f0fe-4cad-acfa-b60f071ae787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import logging\n",
    "import sys\n",
    "import dotenv\n",
    "\n",
    "from huggingface_hub import login, HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c020461-8ee5-4de2-a2d5-67742038de8b",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0ba441-155f-46f4-a609-31a413747985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5157c5-3e54-4676-8fd9-74d0fa494dc7",
   "metadata": {},
   "source": [
    "### Env Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24fc333-9519-4c65-8f66-14a93ed993bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-03 21:49:21,375 - __main__ - INFO - Looking .env files at current directory...\n",
      "2024-03-03 21:49:21,377 - __main__ - INFO - Loaded .env variables\n",
      "2024-03-03 21:49:21,377 - __main__ - INFO - Loaded .env variables\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Looking .env files at current directory...\")\n",
    "dotenv_exists = dotenv.load_dotenv(dotenv.find_dotenv(usecwd=True))\n",
    "if dotenv_exists == False:\n",
    "    logger.warning(\"No .env found! Prompting user for credentials...\")\n",
    "    _hugginface_token = getpass.getpass(\"Enter HUGGINGFACE TOKEN: \")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\") if dotenv_exists == True else _hugginface_token\n",
    "logger.info(\"Loaded .env variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c45fc-250f-4d03-b90b-ff95f57bb63c",
   "metadata": {},
   "source": [
    "## Step 1 - Clone llama.cpp repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774f8cd8-eba0-42e0-b528-8ba18bde108f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 19504, done.\u001b[K\n",
      "remote: Counting objects: 100% (11747/11747), done.\u001b[K\n",
      "remote: Compressing objects: 100% (639/639), done.\u001b[K\n",
      "remote: Total 19504 (delta 11327), reused 11176 (delta 11107), pack-reused 7757\u001b[K\n",
      "Receiving objects: 100% (19504/19504), 20.40 MiB | 12.90 MiB/s, done.\n",
      "Resolving deltas: 100% (13945/13945), done.\n"
     ]
    }
   ],
   "source": [
    "! rm -rf llama.cpp && \\\n",
    "    git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688d461-9cd4-4de5-adab-c75f77b2dad2",
   "metadata": {},
   "source": [
    "## Step 2 - Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760b859c-d7cf-4a20-97c2-f4fa0d1d3bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
      "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.6 kB)\n",
      "Collecting sentencepiece~=0.1.98 (from -r llama.cpp/./requirements/requirements-convert.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /opt/conda/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)\n",
      "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
      "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading torch-2.1.2-cp311-cp311-manylinux2014_aarch64.whl.metadata (25 kB)\n",
      "Collecting einops~=0.7.0 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.10.0)\n",
      "Collecting sympy (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_aarch64.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.7/293.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp311-cp311-manylinux2014_aarch64.whl (84.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (29 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, mpmath, sympy, protobuf, numpy, networkx, MarkupSafe, einops, jinja2, gguf, torch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed MarkupSafe-2.1.5 einops-0.7.0 gguf-0.6.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.24.4 protobuf-4.25.3 sentencepiece-0.1.99 sympy-1.12 torch-2.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203215dd-dbf4-4149-99b3-1bdd6da2428b",
   "metadata": {},
   "source": [
    "## Step 3 - Converting Fine-tuned adapters model to GGML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658f83c0-88d3-4db3-8006-a6ef7cd8bb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight => blk.0.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight => blk.0.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight => blk.0.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight => blk.0.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight => blk.1.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight => blk.1.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight => blk.1.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight => blk.1.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight => blk.2.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight => blk.2.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight => blk.2.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight => blk.2.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight => blk.3.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight => blk.3.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight => blk.3.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight => blk.3.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight => blk.4.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight => blk.4.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight => blk.4.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight => blk.4.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight => blk.5.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight => blk.5.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight => blk.5.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight => blk.5.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight => blk.6.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight => blk.6.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight => blk.6.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight => blk.6.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight => blk.7.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight => blk.7.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight => blk.7.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight => blk.7.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight => blk.8.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight => blk.8.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight => blk.8.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight => blk.8.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight => blk.9.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight => blk.9.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight => blk.9.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight => blk.9.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight => blk.10.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight => blk.10.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight => blk.10.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight => blk.10.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight => blk.11.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight => blk.11.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight => blk.11.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight => blk.11.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight => blk.12.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight => blk.12.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight => blk.12.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight => blk.12.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight => blk.13.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight => blk.13.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight => blk.13.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight => blk.13.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight => blk.14.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight => blk.14.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight => blk.14.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight => blk.14.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight => blk.15.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight => blk.15.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight => blk.15.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight => blk.15.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight => blk.16.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight => blk.16.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight => blk.16.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight => blk.16.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight => blk.17.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight => blk.17.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight => blk.17.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight => blk.17.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight => blk.18.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight => blk.18.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight => blk.18.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight => blk.18.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight => blk.19.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight => blk.19.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight => blk.19.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight => blk.19.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight => blk.20.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight => blk.20.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight => blk.20.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight => blk.20.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight => blk.21.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight => blk.21.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight => blk.21.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight => blk.21.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight => blk.22.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight => blk.22.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight => blk.22.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight => blk.22.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight => blk.23.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight => blk.23.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight => blk.23.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight => blk.23.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight => blk.24.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight => blk.24.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight => blk.24.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight => blk.24.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight => blk.25.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight => blk.25.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight => blk.25.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight => blk.25.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight => blk.26.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight => blk.26.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight => blk.26.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight => blk.26.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight => blk.27.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight => blk.27.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight => blk.27.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight => blk.27.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight => blk.28.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight => blk.28.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight => blk.28.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight => blk.28.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight => blk.29.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight => blk.29.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight => blk.29.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight => blk.29.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight => blk.30.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight => blk.30.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight => blk.30.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight => blk.30.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight => blk.31.attn_q.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight => blk.31.attn_q.weight.loraB (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight => blk.31.attn_v.weight.loraA (4096, 64) float32 1.00MB\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight => blk.31.attn_v.weight.loraB (4096, 64) float32 1.00MB\n",
      "Converted gguf-fine-tuned-chat-model/adapter_config.json and gguf-fine-tuned-chat-model/adapter_model.bin to gguf-fine-tuned-chat-model/ggml-adapter-model.bin\n"
     ]
    }
   ],
   "source": [
    "# !python llama.cpp/convert-lora-to-ggml.py 'gguf-fine-tuned-chat-model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ebf05-77cd-4dce-a08a-74ec253b77bb",
   "metadata": {},
   "source": [
    "## Step 4 - Convert model into f16 or f32 GGUF versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66855b6b-fdea-4438-88f1-0680cc49b264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: convert.py [-h] [--awq-path AWQ_PATH] [--dump] [--dump-single]\n",
      "                  [--vocab-only] [--outtype {f32,f16,q8_0}]\n",
      "                  [--vocab-dir VOCAB_DIR] [--vocab-type VOCAB_TYPE]\n",
      "                  [--outfile OUTFILE] [--ctx CTX] [--concurrency CONCURRENCY]\n",
      "                  [--big-endian] [--pad-vocab] [--skip-unknown]\n",
      "                  model\n",
      "\n",
      "Convert a LLaMA model to a GGML compatible file\n",
      "\n",
      "positional arguments:\n",
      "  model                 directory containing model file, or model file itself\n",
      "                        (*.pth, *.pt, *.bin)\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --awq-path AWQ_PATH   Path to scale awq cache file\n",
      "  --dump                don't convert, just show what's in the model\n",
      "  --dump-single         don't convert, just show what's in a single model file\n",
      "  --vocab-only          extract only the vocab\n",
      "  --outtype {f32,f16,q8_0}\n",
      "                        output format - note: q8_0 may be very slow (default:\n",
      "                        f16 or f32 based on input)\n",
      "  --vocab-dir VOCAB_DIR\n",
      "                        directory containing tokenizer.model, if separate from\n",
      "                        model file\n",
      "  --vocab-type VOCAB_TYPE\n",
      "                        vocab types to try in order, choose from 'spm', 'bpe',\n",
      "                        'hfft' (default: spm,hfft)\n",
      "  --outfile OUTFILE     path to write to; default: based on input\n",
      "  --ctx CTX             model training context (default: based on input)\n",
      "  --concurrency CONCURRENCY\n",
      "                        concurrency used for conversion (default: 8)\n",
      "  --big-endian          model is executed on big endian machine\n",
      "  --pad-vocab           add pad tokens when model vocab expects more than\n",
      "                        tokenizer metadata provides\n",
      "  --skip-unknown        skip unknown tensor names instead of failing\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88c5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GGML_OUTPUT_DIR=./models/Llama-2-7b-chat-hf\n",
      "./models/Llama-2-7b-chat-hf\n",
      "/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "# Store relative path as an ENV VAR\n",
    "%env GGML_OUTPUT_DIR=./models/Llama-2-7b-chat-hf\n",
    "!echo $GGML_OUTPUT_DIR\n",
    "!realpath $GGML_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83df0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "# Convert ENV VAR relative path to absolute path\n",
    "os.environ[\"GGML_OUTPUT_DIR\"] = os.popen(\"realpath $GGML_OUTPUT_DIR\").read().strip()\n",
    "print(os.environ[\"GGML_OUTPUT_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2eec25-6992-4f34-ac7c-a42ca5d612cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/Llama-2-7b-chat-hf/model-00001-of-00002.safetensors\n",
      "Loading model file models/Llama-2-7b-chat-hf/model-00001-of-00002.safetensors\n",
      "Loading model file models/Llama-2-7b-chat-hf/model-00002-of-00002.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models/Llama-2-7b-chat-hf'))\n",
      "Found vocab files: {'spm': PosixPath('models/Llama-2-7b-chat-hf/tokenizer.model'), 'bpe': None, 'hfft': PosixPath('models/Llama-2-7b-chat-hf/tokenizer.json')}\n",
      "Loading vocab file PosixPath('models/Llama-2-7b-chat-hf/tokenizer.model'), type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.0.attn_rot_embd\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.1.attn_rot_embd\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.10.attn_rot_embd\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.11.attn_rot_embd\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.12.attn_rot_embd\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.13.attn_rot_embd\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.14.attn_rot_embd\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.15.attn_rot_embd\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.16.attn_rot_embd\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.17.attn_rot_embd\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.18.attn_rot_embd\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.19.attn_rot_embd\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.2.attn_rot_embd\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.20.attn_rot_embd\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.21.attn_rot_embd\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.22.attn_rot_embd\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.23.attn_rot_embd\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.3.attn_rot_embd\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.4.attn_rot_embd\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.5.attn_rot_embd\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.6.attn_rot_embd\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.7.attn_rot_embd\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.8.attn_rot_embd\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
      "skipping tensor blk.9.attn_rot_embd\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.24.attn_rot_embd\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.25.attn_rot_embd\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.26.attn_rot_embd\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.27.attn_rot_embd\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.28.attn_rot_embd\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.29.attn_rot_embd\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.30.attn_rot_embd\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
      "skipping tensor blk.31.attn_rot_embd\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
      "Writing models/Llama-2-7b-chat-hf/ggml-model-f16.gguf, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "gguf: Setting chat_template to {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   5\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   8\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   9\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  10\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  10\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  10\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+  11\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  12\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  12\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  13\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  14\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  15\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  15\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  15\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  15\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  17\n",
      "[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  17\n",
      "[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  17\n",
      "[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  18\n",
      "[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  19\n",
      "[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  20\n",
      "[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  20\n",
      "[ 26/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  20\n",
      "[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  20\n",
      "[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  21\n",
      "[ 29/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  21\n",
      "[ 30/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  21\n",
      "[ 31/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  23\n",
      "[ 32/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  25\n",
      "[ 33/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  28\n",
      "[ 34/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  28\n",
      "[ 35/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  31\n",
      "[ 36/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  31\n",
      "[ 37/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  32\n",
      "[ 38/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  32\n",
      "[ 39/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  32\n",
      "[ 40/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  34\n",
      "[ 41/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  38\n",
      "[ 42/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  39\n",
      "[ 43/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  39\n",
      "[ 44/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  41\n",
      "[ 45/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
      "[ 46/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
      "[ 47/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[ 48/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  43\n",
      "[ 49/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  44\n",
      "[ 50/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  44\n",
      "[ 51/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[ 52/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  47\n",
      "[ 53/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  47\n",
      "[ 54/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  47\n",
      "[ 55/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  48\n",
      "[ 56/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  48\n",
      "[ 57/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  48\n",
      "[ 58/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  49\n",
      "[ 59/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  49\n",
      "[ 60/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
      "[ 61/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  50\n",
      "[ 62/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  50\n",
      "[ 63/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  50\n",
      "[ 64/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  50\n",
      "[ 65/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  51\n",
      "[ 66/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  51\n",
      "[ 67/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  52\n",
      "[ 68/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  53\n",
      "[ 69/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[ 70/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
      "[ 71/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  56\n",
      "[ 72/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  57\n",
      "[ 73/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  57\n",
      "[ 74/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  58\n",
      "[ 75/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  58\n",
      "[ 76/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  58\n",
      "[ 77/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  59\n",
      "[ 78/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  59\n",
      "[ 79/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  59\n",
      "[ 80/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  59\n",
      "[ 81/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  60\n",
      "[ 82/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
      "[ 83/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
      "[ 84/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  61\n",
      "[ 85/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  62\n",
      "[ 86/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  62\n",
      "[ 87/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  63\n",
      "[ 88/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  63\n",
      "[ 89/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  63\n",
      "[ 90/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  64\n",
      "[ 91/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  65\n",
      "[ 92/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  66\n",
      "[ 93/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  66\n",
      "[ 94/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  67\n",
      "[ 95/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  68\n",
      "[ 96/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  70\n",
      "[ 97/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  70\n",
      "[ 98/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  71\n",
      "[ 99/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  71\n",
      "[100/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  72\n",
      "[101/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[102/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  72\n",
      "[103/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  73\n",
      "[104/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  74\n",
      "[105/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
      "[106/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  75\n",
      "[107/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  76\n",
      "[108/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  77\n",
      "[109/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  77\n",
      "[110/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  77\n",
      "[111/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  77\n",
      "[112/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  78\n",
      "[113/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  78\n",
      "[114/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  78\n",
      "[115/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  78\n",
      "[116/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  79\n",
      "[117/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  79\n",
      "[118/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  79\n",
      "[119/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
      "[120/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  80\n",
      "[121/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  82\n",
      "[122/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  83\n",
      "[123/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  83\n",
      "[124/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  83\n",
      "[125/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  84\n",
      "[126/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  84\n",
      "[127/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  84\n",
      "[128/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  85\n",
      "[129/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  85\n",
      "[130/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  86\n",
      "[131/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  87\n",
      "[132/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
      "[133/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  89\n",
      "[134/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  89\n",
      "[135/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  92\n",
      "[136/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  93\n",
      "[137/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  94\n",
      "[138/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  94\n",
      "[139/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  97\n",
      "[140/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  99\n",
      "[141/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 102\n",
      "[142/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 102\n",
      "[143/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 102\n",
      "[144/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 102\n",
      "[145/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 103\n",
      "[146/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 103\n",
      "[147/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 103\n",
      "[148/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 105\n",
      "[149/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 105\n",
      "[150/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 106\n",
      "[151/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
      "[152/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 106\n",
      "[153/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
      "[154/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
      "[155/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+ 107\n",
      "[156/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 107\n",
      "[157/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 109\n",
      "[158/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 110\n",
      "[159/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+ 113\n",
      "[160/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 113\n",
      "[161/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 113\n",
      "[162/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 114\n",
      "[163/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 114\n",
      "[164/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+ 114\n",
      "[165/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 114\n",
      "[166/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 115\n",
      "[167/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 119\n",
      "[168/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+ 121\n",
      "[169/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 121\n",
      "[170/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 121\n",
      "[171/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 123\n",
      "[172/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 124\n",
      "[173/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+ 125\n",
      "[174/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 125\n",
      "[175/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 126\n",
      "[176/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 127\n",
      "[177/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+ 128\n",
      "[178/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 128\n",
      "[179/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 128\n",
      "[180/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 129\n",
      "[181/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 129\n",
      "[182/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+ 130\n",
      "[183/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 130\n",
      "[184/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 131\n",
      "[185/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 132\n",
      "[186/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+ 133\n",
      "[187/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 133\n",
      "[188/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 133\n",
      "[189/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 133\n",
      "[190/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 134\n",
      "[191/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 134\n",
      "[192/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 134\n",
      "[193/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 135\n",
      "[194/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 136\n",
      "[195/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 137\n",
      "[196/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 137\n",
      "[197/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 138\n",
      "[198/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 138\n",
      "[199/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 138\n",
      "[200/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 138\n",
      "[201/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 138\n",
      "[202/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 141\n",
      "[203/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 143\n",
      "[204/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 144\n",
      "[205/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 144\n",
      "[206/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 145\n",
      "[207/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 147\n",
      "[208/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 147\n",
      "[209/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 148\n",
      "[210/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+ 148\n",
      "[211/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+ 149\n",
      "[212/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+ 151\n",
      "[213/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 154\n",
      "[214/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+ 154\n",
      "[215/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+ 154\n",
      "[216/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+ 154\n",
      "[217/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+ 155\n",
      "[218/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 155\n",
      "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 159\n",
      "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 159\n",
      "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 160\n",
      "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 160\n",
      "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 161\n",
      "[224/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 161\n",
      "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 162\n",
      "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 163\n",
      "[227/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 163\n",
      "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 163\n",
      "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 163\n",
      "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 164\n",
      "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 165\n",
      "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 168\n",
      "[233/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 168\n",
      "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 168\n",
      "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 169\n",
      "[236/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 169\n",
      "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 170\n",
      "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 170\n",
      "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 170\n",
      "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 173\n",
      "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 174\n",
      "[242/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 174\n",
      "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 174\n",
      "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 175\n",
      "[245/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 175\n",
      "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 176\n",
      "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 176\n",
      "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 178\n",
      "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 178\n",
      "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 179\n",
      "[251/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 179\n",
      "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 179\n",
      "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 180\n",
      "[254/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 180\n",
      "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 180\n",
      "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 180\n",
      "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 181\n",
      "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 181\n",
      "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 182\n",
      "[260/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 182\n",
      "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 182\n",
      "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 182\n",
      "[263/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 182\n",
      "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 183\n",
      "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 183\n",
      "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 184\n",
      "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 185\n",
      "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 187\n",
      "[269/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 187\n",
      "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 189\n",
      "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 189\n",
      "[272/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 190\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 191\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 191\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 193\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 194\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 195\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 195\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 195\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 195\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 196\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 196\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 196\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 197\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 198\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 198\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 198\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 198\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 198\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 198\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 198\n",
      "Wrote models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert.py $GGML_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2689a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GGML_MODEL_PATH=./models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\n",
      "./models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\n",
      "/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Store relative path as an ENV VAR\n",
    "%env GGML_MODEL_PATH=./models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\n",
    "!echo $GGML_MODEL_PATH\n",
    "!realpath $GGML_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e2eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Convert ENV VAR relative path to absolute path\n",
    "os.environ[\"GGML_MODEL_PATH\"] = os.popen(\"realpath $GGML_MODEL_PATH\").read().strip()\n",
    "print(os.environ[\"GGML_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b0907-a7c9-47e1-8fae-08ad48c4a9cc",
   "metadata": {},
   "source": [
    "## Step 5 - Quantize model to 4bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a36167b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "cmake is already the newest version (3.25.1-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "-- The C compiler identification is GNU 12.2.0\n",
      "-- The CXX compiler identification is GNU 12.2.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.39.2\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: aarch64\n",
      "-- ARM detected\n",
      "-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\n",
      "-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /workspaces/Llama_to_Llama.cpp/llama.cpp/build\n",
      "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
      "[  4%] Built target ggml\n",
      "[  4%] \u001b[32m\u001b[1mLinking C static library libggml_static.a\u001b[0m\n",
      "[  4%] Built target ggml_static\n",
      "[  5%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
      "[  6%] Built target llama\n",
      "[  7%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  7%] Built target build_info\n",
      "[  8%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 12%] Built target common\n",
      "[ 12%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 14%] Built target test-quantize-fns\n",
      "[ 15%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 16%] Built target test-quantize-perf\n",
      "[ 17%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 18%] Built target test-sampling\n",
      "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 20%] Built target test-chat-template\n",
      "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
      "[ 22%] Built target test-tokenizer-0-llama\n",
      "[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
      "[ 25%] Built target test-tokenizer-0-falcon\n",
      "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
      "[ 28%] Built target test-tokenizer-1-llama\n",
      "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 30%] Built target test-tokenizer-1-bpe\n",
      "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 33%] Built target test-grammar-parser\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 35%] Built target test-llama-grammar\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
      "[ 37%] Built target test-grad0\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 39%] Built target test-backend-ops\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 42%] Built target test-rope\n",
      "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 45%] Built target test-model-load-cancel\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 48%] Built target test-autorelease\n",
      "[ 49%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
      "[ 50%] Built target test-c\n",
      "[ 50%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
      "[ 51%] Built target baby-llama\n",
      "[ 52%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
      "[ 53%] Built target batched\n",
      "[ 54%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
      "[ 54%] Built target batched-bench\n",
      "[ 55%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
      "[ 56%] Built target beam-search\n",
      "[ 57%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
      "[ 58%] Built target benchmark\n",
      "[ 59%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
      "[ 60%] Built target convert-llama2c-to-ggml\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
      "[ 61%] Built target embedding\n",
      "[ 62%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
      "[ 62%] Built target finetune\n",
      "[ 62%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
      "[ 63%] Built target infill\n",
      "[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 64%] Built target llama-bench\n",
      "[ 65%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "[ 66%] Built target llava\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "[ 66%] Built target llava_static\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
      "[ 68%] Built target llava-cli\n",
      "[ 68%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
      "[ 69%] Built target main\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/tokenize\u001b[0m\n",
      "[ 70%] Built target tokenize\n",
      "[ 71%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
      "[ 72%] Built target parallel\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
      "[ 74%] Built target perplexity\n",
      "[ 74%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
      "[ 75%] Built target quantize\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
      "[ 77%] Built target quantize-stats\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
      "[ 78%] Built target save-load-state\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
      "[ 80%] Built target simple\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/passkey\u001b[0m\n",
      "[ 81%] Built target passkey\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
      "[ 82%] Built target speculative\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookahead\u001b[0m\n",
      "[ 84%] Built target lookahead\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookup\u001b[0m\n",
      "[ 86%] Built target lookup\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gguf\u001b[0m\n",
      "[ 88%] Built target gguf\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
      "[ 90%] Built target train-text-from-scratch\n",
      "[ 91%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/imatrix\u001b[0m\n",
      "[ 92%] Built target imatrix\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
      "[ 94%] Built target server\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
      "[ 96%] Built target export-lora\n",
      "[ 97%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
      "[ 98%] Built target vdot\n",
      "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
      "[100%] Built target q8dot\n"
     ]
    }
   ],
   "source": [
    "!apt-get -y install cmake && \\\n",
    "    cd llama.cpp && \\\n",
    "    mkdir build && \\\n",
    "    cd build && \\\n",
    "    cmake .. && \\\n",
    "    cmake --build . --config Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372e2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q4_0\n",
      "/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf/ggml-model-q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# Store quantization option and path as ENV VARs\n",
    "os.environ[\"QUANTIZE_OPTION\"] = \"q4_0\"\n",
    "os.environ[\"GGML_QUANTIZE_MODEL_PATH\"] = (\n",
    "    f\"{os.popen('realpath $GGML_OUTPUT_DIR').read().strip()}\"\n",
    "    f\"/ggml-model-{os.environ['QUANTIZE_OPTION']}.gguf\"\n",
    ")\n",
    "print(os.environ[\"QUANTIZE_OPTION\"])\n",
    "print(os.environ[\"GGML_QUANTIZE_MODEL_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6c7fdc6-79be-4e57-a68e-6f27dbede32b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 2321 (97311342)\n",
      "main: built with cc (Debian 12.2.0-14) 12.2.0 for aarch64-linux-gnu\n",
      "main: quantizing '/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf/ggml-model-f16.gguf' to '/workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf/ggml-model-q4_0.gguf' as Q4_0\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /workspaces/Llama_to_Llama.cpp/models/Llama-2-7b-chat-hf/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 741984 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.034 0.008 0.013 0.021 0.033 0.054 0.089 0.150 0.225 0.151 0.089 0.054 0.033 0.021 0.013 0.011 \n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.011 0.018 0.028 0.044 0.068 0.100 0.135 0.154 0.135 0.100 0.068 0.044 0.028 0.017 0.014 \n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.034 0.008 0.012 0.019 0.031 0.050 0.084 0.149 0.255 0.150 0.084 0.051 0.031 0.019 0.012 0.010 \n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.036 0.053 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.024 0.020 \n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.022 0.034 0.051 0.074 0.099 0.121 0.132 0.121 0.099 0.074 0.051 0.034 0.022 0.018 \n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.031 0.047 0.070 0.099 0.129 0.145 0.129 0.099 0.070 0.047 0.031 0.020 0.016 \n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.022 0.034 0.052 0.074 0.098 0.121 0.132 0.121 0.098 0.074 0.052 0.034 0.022 0.018 \n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.014 0.023 0.035 0.052 0.074 0.097 0.119 0.130 0.119 0.097 0.074 0.053 0.035 0.023 0.019 \n",
      "[  20/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  22/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  23/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  24/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  27/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  28/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  29/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  31/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  32/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  36/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  38/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  40/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  42/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  45/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  46/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  47/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  49/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  51/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  52/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  53/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  54/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  55/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  56/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  58/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  62/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  63/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  64/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  65/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  67/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  71/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  72/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  73/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  74/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  76/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  78/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  80/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  81/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  82/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  83/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  85/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  89/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  90/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  91/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  92/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  94/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  96/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  98/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  99/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 100/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 103/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 107/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 108/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 110/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 112/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 116/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 117/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 118/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.120 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 121/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 123/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 125/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 126/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 127/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 130/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 134/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 135/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 136/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 139/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 143/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 144/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 145/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 148/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 152/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 153/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 154/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 157/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 161/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 162/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 163/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 164/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 166/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 170/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 171/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 172/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 173/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 175/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 179/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 180/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 181/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 182/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 184/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 188/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 189/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 190/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 191/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 193/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 197/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 198/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 199/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 200/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 202/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 206/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 207/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 208/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 209/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 211/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 215/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 216/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 217/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 218/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB\n",
      "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.023 0.036 0.054 0.075 0.098 0.116 0.124 0.116 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 112653.52 ms\n",
      "main:    total time = 112653.52 ms\n"
     ]
    }
   ],
   "source": [
    "!cd llama.cpp/build/bin && \\\n",
    "    ./quantize $GGML_MODEL_PATH $GGML_QUANTIZE_MODEL_PATH $QUANTIZE_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4c5fd-66e3-43ab-84c2-eecfb08431f7",
   "metadata": {},
   "source": [
    "## Optional: Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "880e57fe-0edd-4015-a915-064b4114f742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub[\"cli\"] --upgrade --user # already inclueded in dev container environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255bd878-2afc-4607-bd8d-abe4f35ca778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2048219987.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    allow_patterns=\"*.gguf\"     # Remove if you would like to uplodad different file types.\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Logging to Hugging Face...\")\n",
    "HUGGINGFACE_USER = os.getenv(\"HUGGINGFACE_USER\")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=HUGGINGFACE_TOKEN, write_permission=True)\n",
    "\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "REPO_ID = f\"{HUGGINGFACE_USER}/llama-2-7b-chat-{os.environ['QUANTIZE_OPTION']}.gguf\"\n",
    "logger.info(\"Creating repo %s in Hugging Face...\", REPO_ID)\n",
    "api.create_repo(\n",
    "    repo_id=REPO_ID,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "logger.info(\"Uploading fine-tuned / quantize model to %s in Hugging Face...\", REPO_ID)\n",
    "api.upload_folder(\n",
    "    repo_id=REPO_ID,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    repo_type=\"model\",\n",
    "    folder_path=os.environ[\"GGML_OUTPUT_DIR\"],\n",
    "    allow_patterns=\"*.gguf\",    # Remove if you would like to uplodad different file types.\n",
    "    multi_commits=True,         # For large file uploads, refer: https://huggingface.co/docs/huggingface_hub/guides/upload\n",
    "    multi_commits_verbose=True, # For large file uploads, refer: https://huggingface.co/docs/huggingface_hub/guides/upload\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
